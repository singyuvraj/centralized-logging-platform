# Handoff: Logging stack and run instructions

This document is for the reviewer or manager who needs to understand what was built, how to run it, and what is left out of scope.

---

## What this system is

A centralized logging pipeline for the suljhaoo backend. Application logs are written to a file, shipped by Filebeat to Logstash over TLS, buffered and optionally enriched in Logstash, stored in Elasticsearch, and queried via Kibana. NGINX terminates HTTPS in front of Kibana. No log data is stored in Git; passwords and TLS keys are local-only.

Flow: **Backend** (writes to `/apps/logs/*.log`) → **Filebeat** (sidecar, TLS) → **Logstash** (Persistent Queue, Dead Letter Queue) → **Elasticsearch** (HTTPS, no public ports) → **Kibana** (HTTPS via nginx).

---

## What was implemented (by phase)

- **Phase 0:** Contract: log path `/apps/logs/*.log`, structured JSON (NDJSON), mandatory fields, sidecar Filebeat, Logstash in the middle.
- **Phase 1:** Application logging: Log4j2, ECS-style JSON, file and console, trace ID in MDC, path `/apps/logs/application.log`.
- **Phase 2:** Filebeat sidecar: shared volume with backend, TLS to Logstash, registry persisted, field contract (identity from app only).
- **Phase 3:** Logstash: Beats input on 5044 with TLS, stdout verification (no Elasticsearch yet).
- **Phase 4:** Elasticsearch: single-node, security on, HTTPS only, no host ports; `logstash_writer` role/user; index per service per day (`<service.name>-logs-YYYY.MM.dd`).
- **Phase 5:** Kibana behind nginx: HTTPS on 443, `kibana_system` for Kibana→Elasticsearch, `kibana_user` with read-only access to log indices.
- **Reliability:** Logstash Persistent Queue (PQ) and Dead Letter Queue (DLQ) on disk; TLS end-to-end where specified; certificates generated by `tls/gen-certs.sh`.

---

## What is not implemented (intentionally)

- Index Lifecycle Management (ILM), rollover, warm/cold tiers.
- Kibana dashboards, saved searches, or index patterns (beyond defaults).
- Alerting, Watcher, or on-call runbooks.
- Mutual TLS (client certificates).
- Production PKI or certificate rotation.

These are out of scope for this handoff; the stack is runnable and testable as-is for review.

---

## Exact run commands (copy-paste)

From a fresh clone, with Docker and Docker Compose installed:

```bash
git clone https://github.com/singyuvi46/centralized-logging-platform.git
cd centralized-logging-platform

cp env.example .env
chmod +x tls/gen-certs.sh && ./tls/gen-certs.sh
docker compose up -d
```

Verification:

```bash
docker compose ps
```

Then open `https://localhost` in a browser; accept the self-signed cert and log in to Kibana with the credentials you set in `.env` for the `kibana_user` account (variable `KIBANA_USER_PASSWORD`).

Full step-by-step with explanations and success criteria: see the root **README.md**, section "Run locally (logging stack)".

---

## What someone can test immediately

1. **Containers:** `docker compose ps` — all logging-related services (elasticsearch, logstash, filebeat, kibana, nginx) should be up.
2. **Kibana:** Open `https://localhost`, log in with `kibana_user` and the password from `.env` (`KIBANA_USER_PASSWORD`). You can open Discover and search log indices (pattern `*-logs-*`).
3. **Elasticsearch:** From the host, run the `curl` command in README (step 5) to check cluster health; use the same password as in `.env` for `ELASTIC_PASSWORD`.
4. **Log flow:** The backend (if running) writes to `/apps/logs/application.log`; Filebeat tails it and sends to Logstash; Logstash writes to Elasticsearch. New log lines should appear in Kibana after a short delay.

The backend container may restart until a database is configured; that is separate from the logging stack.

---

## Passwords and secrets the manager must provide locally

Nothing is required from the author. The manager creates a local `.env` from `env.example` and sets values on their machine only.

| Variable | Purpose | Where it is used |
|----------|---------|------------------|
| `ELASTIC_PASSWORD` | Bootstrap password for Elasticsearch `elastic` user | Elasticsearch, es-setup |
| `LOGSTASH_WRITER_PASSWORD` | Password for `logstash_writer` user | es-setup, Logstash output |
| `KIBANA_SYSTEM_PASSWORD` | Password for `kibana_system` (Kibana→Elasticsearch) | es-setup, Kibana |
| `KIBANA_USER_PASSWORD` | Password for `kibana_user` (Kibana UI login) | es-setup, used by human to log in to Kibana |

`env.example` contains placeholder values (e.g. `changeme-elastic`) so that after `cp env.example .env` the stack starts without further edits. For any non-local use, the manager should replace them with strong passwords. `.env` is gitignored and must never be committed. No passwords or secrets are stored in the repository.

---

## Where things live

- **Run instructions:** Root **README.md**, section "Run locally (logging stack)".
- **Architecture and design:** `docs/logging/END_TO_END_LOGGING_SYSTEM_EXPLAINED.md`, plus phase docs in `docs/logging/` (PHASE0–PHASE5, TLS, testing, solution, thinking).
- **Environment template:** `env.example`. Copy to `.env` and set values locally.
- **TLS generation:** `tls/gen-certs.sh`. Run once before first `docker compose up`.
- **Compose definition:** `docker-compose.yml`. Defines backend, filebeat, logstash, elasticsearch, es-setup, kibana, nginx.

---


